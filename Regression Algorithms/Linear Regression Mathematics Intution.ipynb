{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "72d5aa7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d516935a",
   "metadata": {},
   "source": [
    "# Linear Regression Mathematical Intution (Cost Function And Gradient Descent):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49be2892",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Cost: 2331.818181818182, m: 0.6879054545454546, b: 0.004025454545454546, cost: 2331.818181818182\n",
      "Epoch 10001, Cost: 33.75617858050655, m: 0.4027556809256097, b: -0.32318588368371626, cost: 33.75617858050655\n",
      "Epoch 20001, Cost: 33.5805526049421, m: 0.4046706032688362, b: -0.647796120454235, cost: 33.5805526049421\n",
      "Epoch 30001, Cost: 33.40590465178482, m: 0.4065801862777384, b: -0.9715012539001637, cost: 33.40590465178482\n",
      "Epoch 40001, Cost: 33.23222927464314, m: 0.40848444483986035, b: -1.2943038077004496, cost: 33.23222927464314\n",
      "Epoch 50001, Cost: 33.05952105745533, m: 0.4103833938012359, b: -1.6162062984973495, cost: 33.05952105745533\n",
      "Epoch 60001, Cost: 32.887774614320406, m: 0.41227704796650355, b: -1.937211235916029, cost: 32.887774614320406\n",
      "Epoch 70001, Cost: 32.71698458933034, m: 0.4141654220990228, b: -2.2573211225841123, cost: 32.71698458933034\n",
      "Epoch 80001, Cost: 32.54714565640296, m: 0.41604853092098854, b: -2.576538454151207, cost: 32.54714565640296\n",
      "Epoch 90001, Cost: 32.37825251911589, m: 0.4179263891135463, b: -2.8948657193083656, cost: 32.37825251911589\n",
      "Epoch 100001, Cost: 32.210299910541345, m: 0.4197990113169063, b: -3.2123053998074793, cost: 32.210299910541345\n",
      "Epoch 110001, Cost: 32.043282593081926, m: 0.42166641213045764, b: -3.5288599704805854, cost: 32.043282593081926\n",
      "Epoch 120001, Cost: 31.877195358307223, m: 0.42352860611288307, b: -3.8445318992592847, cost: 31.877195358307223\n",
      "Epoch 130001, Cost: 31.71203302679146, m: 0.42538560778227047, b: -4.159323647193797, cost: 31.71203302679146\n",
      "Epoch 140001, Cost: 31.547790447952, m: 0.42723743161622824, b: -4.473237668472344, cost: 31.547790447952\n",
      "Epoch 150001, Cost: 31.38446249988849, m: 0.4290840920519974, b: -4.7862764104401965, cost: 31.38446249988849\n",
      "Epoch 160001, Cost: 31.222044089223512, m: 0.43092560348656267, b: -5.098442313618642, cost: 31.222044089223512\n",
      "Epoch 170001, Cost: 31.060530150943407, m: 0.4327619802767668, b: -5.409737811724161, cost: 31.060530150943407\n",
      "Epoch 180001, Cost: 30.899915648240547, m: 0.43459323673942213, b: -5.720165331687441, cost: 30.899915648240547\n",
      "Epoch 190001, Cost: 30.740195572356104, m: 0.4364193871514207, b: -6.02972729367211, cost: 30.740195572356104\n",
      "Epoch 200001, Cost: 30.58136494242403, m: 0.4382404457498476, b: -6.338426111093785, cost: 30.58136494242403\n",
      "Epoch 210001, Cost: 30.42341880531547, m: 0.4400564267320908, b: -6.646264190638829, cost: 30.42341880531547\n",
      "Epoch 220001, Cost: 30.26635223548468, m: 0.44186734425595103, b: -6.953243932282927, cost: 30.26635223548468\n",
      "Epoch 230001, Cost: 30.11016033481494, m: 0.4436732124397544, b: -7.2593677293101395, cost: 30.11016033481494\n",
      "Epoch 240001, Cost: 29.954838232466237, m: 0.44547404536246055, b: -7.564637968331356, cost: 29.954838232466237\n",
      "Epoch 250001, Cost: 29.800381084723114, m: 0.44726985706377315, b: -7.869057029302869, cost: 29.800381084723114\n",
      "Epoch 260001, Cost: 29.64678407484371, m: 0.4490606615442499, b: -8.172627285545124, cost: 29.64678407484371\n",
      "Epoch 270001, Cost: 29.49404241290963, m: 0.45084647276540996, b: -8.475351103760907, cost: 29.49404241290963\n",
      "Epoch 280001, Cost: 29.34215133567637, m: 0.4526273046498444, b: -8.777230844054044, cost: 29.34215133567637\n",
      "Epoch 290001, Cost: 29.191106106424876, m: 0.4544031710813245, b: -9.078268859947853, cost: 29.191106106424876\n",
      "Epoch 300001, Cost: 29.040902014813945, m: 0.45617408590491026, b: -9.378467498403353, cost: 29.040902014813945\n",
      "Epoch 310001, Cost: 28.891534376733127, m: 0.4579400629270561, b: -9.677829099837362, cost: 28.891534376733127\n",
      "Epoch 320001, Cost: 28.742998534156865, m: 0.4597011159157213, b: -9.976355998141175, cost: 28.742998534156865\n",
      "Epoch 330001, Cost: 28.59528985499894, m: 0.46145725860047704, b: -10.274050520698623, cost: 28.59528985499894\n",
      "Epoch 340001, Cost: 28.44840373296844, m: 0.4632085046726109, b: -10.570914988403855, cost: 28.44840373296844\n",
      "Epoch 350001, Cost: 28.30233558742566, m: 0.46495486778523654, b: -10.866951715680006, cost: 28.30233558742566\n",
      "Epoch 360001, Cost: 28.15708086323959, m: 0.46669636155339855, b: -11.162163010496798, cost: 28.15708086323959\n",
      "Epoch 370001, Cost: 28.012635030645676, m: 0.46843299955417944, b: -11.456551174388768, cost: 28.012635030645676\n",
      "Epoch 380001, Cost: 27.868993585104672, m: 0.47016479532680505, b: -11.750118502473207, cost: 27.868993585104672\n",
      "Epoch 390001, Cost: 27.726152047162063, m: 0.4718917623727492, b: -12.042867283467787, cost: 27.726152047162063\n",
      "Epoch 400001, Cost: 27.584105962308577, m: 0.4736139141558405, b: -12.334799799708684, cost: 27.584105962308577\n",
      "Epoch 410001, Cost: 27.44285090084103, m: 0.4753312641023656, b: -12.625918327168172, cost: 27.44285090084103\n",
      "Epoch 420001, Cost: 27.30238245772415, m: 0.477043825601178, b: -12.916225135472953, cost: 27.30238245772415\n",
      "Epoch 430001, Cost: 27.162696252453507, m: 0.4787516120037955, b: -13.20572248792073, cost: 27.162696252453507\n",
      "Epoch 440001, Cost: 27.02378792891868, m: 0.4804546366245114, b: -13.494412641499059, cost: 27.02378792891868\n",
      "Epoch 450001, Cost: 26.885653155267455, m: 0.4821529127404932, b: -13.782297846902072, cost: 26.885653155267455\n",
      "Epoch 460001, Cost: 26.748287623770814, m: 0.4838464535918882, b: -14.069380348548394, cost: 26.748287623770814\n",
      "Epoch 470001, Cost: 26.611687050688474, m: 0.4855352723819268, b: -14.355662384598606, cost: 26.611687050688474\n",
      "Epoch 480001, Cost: 26.475847176135392, m: 0.48721938227702455, b: -14.641146186972636, cost: 26.475847176135392\n",
      "Epoch 490001, Cost: 26.340763763948857, m: 0.4888987964068862, b: -14.925833981367266, cost: 26.340763763948857\n",
      "Epoch 500001, Cost: 26.206432601556397, m: 0.490573527864607, b: -15.209727987273448, cost: 26.206432601556397\n",
      "Epoch 510001, Cost: 26.072849499844455, m: 0.4922435897067758, b: -15.49283041799365, cost: 26.072849499844455\n",
      "Epoch 520001, Cost: 25.940010293027772, m: 0.4939089949535743, b: -15.775143480658762, cost: 25.940010293027772\n",
      "Epoch 530001, Cost: 25.8079108385194, m: 0.4955697565888811, b: -16.056669376245736, cost: 25.8079108385194\n",
      "Epoch 540001, Cost: 25.676547016801464, m: 0.497225887560373, b: -16.337410299594726, cost: 25.676547016801464\n",
      "Epoch 550001, Cost: 25.54591473129691, m: 0.4988774007796234, b: -16.61736843942573, cost: 25.54591473129691\n",
      "Epoch 560001, Cost: 25.41600990824149, m: 0.5005243091222071, b: -16.896545978356475, cost: 25.41600990824149\n",
      "Epoch 570001, Cost: 25.28682849655714, m: 0.502166625427794, b: -17.174945092918154, cost: 25.28682849655714\n",
      "Epoch 580001, Cost: 25.15836646772476, m: 0.5038043625002596, b: -17.452567953574313, cost: 25.15836646772476\n",
      "Epoch 590001, Cost: 25.030619815659755, m: 0.5054375331077742, b: -17.72941672473584, cost: 25.030619815659755\n",
      "Epoch 600001, Cost: 24.903584556586527, m: 0.507066149982906, b: -18.005493564778465, cost: 24.903584556586527\n",
      "Epoch 610001, Cost: 24.777256728913837, m: 0.5086902258227246, b: -18.280800626060408, cost: 24.777256728913837\n",
      "Epoch 620001, Cost: 24.65163239311195, m: 0.5103097732888957, b: -18.5553400549383, cost: 24.65163239311195\n",
      "Epoch 630001, Cost: 24.52670763158949, m: 0.5119248050077798, b: -18.829113991783984, cost: 24.52670763158949\n",
      "Epoch 640001, Cost: 24.40247854857122, m: 0.5135353335705325, b: -19.1021245710015, cost: 24.40247854857122\n",
      "Epoch 650001, Cost: 24.278941269976624, m: 0.515141371533201, b: -19.37437392104349, cost: 24.278941269976624\n",
      "Epoch 660001, Cost: 24.156091943299202, m: 0.5167429314168238, b: -19.64586416442797, cost: 24.156091943299202\n",
      "Epoch 670001, Cost: 24.033926737486137, m: 0.5183400257075248, b: -19.91659741775448, cost: 24.033926737486137\n",
      "Epoch 680001, Cost: 23.912441842818755, m: 0.5199326668566174, b: -20.18657579172152, cost: 23.912441842818755\n",
      "Epoch 690001, Cost: 23.791633470794007, m: 0.5215208672806947, b: -20.455801391141968, cost: 23.791633470794007\n",
      "Epoch 700001, Cost: 23.67149785400631, m: 0.5231046393617279, b: -20.724276314959777, cost: 23.67149785400631\n",
      "Epoch 710001, Cost: 23.5520312460298, m: 0.5246839954471643, b: -20.992002656266486, cost: 23.5520312460298\n",
      "Epoch 720001, Cost: 23.433229921301553, m: 0.526258947850025, b: -21.258982502317863, cost: 23.433229921301553\n",
      "Epoch 730001, Cost: 23.315090175005658, m: 0.5278295088489973, b: -21.52521793454949, cost: 23.315090175005658\n",
      "Epoch 740001, Cost: 23.197608322957343, m: 0.5293956906885329, b: -21.7907110285936, cost: 23.197608322957343\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 750001, Cost: 23.08078070148834, m: 0.5309575055789422, b: -22.05546385429474, cost: 23.08078070148834\n",
      "Epoch 760001, Cost: 22.964603667332575, m: 0.5325149656964898, b: -22.31947847572629, cost: 22.964603667332575\n",
      "Epoch 770001, Cost: 22.849073597512458, m: 0.5340680831834898, b: -22.58275695120643, cost: 22.849073597512458\n",
      "Epoch 780001, Cost: 22.734186889226017, m: 0.5356168701483986, b: -22.845301333313923, cost: 22.734186889226017\n",
      "Epoch 790001, Cost: 22.619939959734413, m: 0.5371613386659138, b: -23.10711366890474, cost: 22.619939959734413\n",
      "Epoch 800001, Cost: 22.50632924625037, m: 0.5387015007770635, b: -23.368195999127547, cost: 22.50632924625037\n",
      "Epoch 810001, Cost: 22.393351205826924, m: 0.540237368489302, b: -23.628550359439586, cost: 22.393351205826924\n",
      "Epoch 820001, Cost: 22.281002315246997, m: 0.5417689537766052, b: -23.888178779622965, cost: 22.281002315246997\n",
      "Epoch 830001, Cost: 22.16927907091364, m: 0.54329626857956, b: -24.147083283799844, cost: 22.16927907091364\n",
      "Epoch 840001, Cost: 22.058177988740532, m: 0.5448193248054606, b: -24.405265890448725, cost: 22.058177988740532\n",
      "Epoch 850001, Cost: 21.947695604043577, m: 0.5463381343284005, b: -24.662728612420093, cost: 21.947695604043577\n",
      "Epoch 860001, Cost: 21.83782847143268, m: 0.5478527089893643, b: -24.919473456951874, cost: 21.83782847143268\n",
      "Epoch 870001, Cost: 21.72857316470431, m: 0.5493630605963223, b: -25.17550242568559, cost: 21.72857316470431\n",
      "Epoch 880001, Cost: 21.61992627673496, m: 0.5508692009243177, b: -25.430817514680996, cost: 21.61992627673496\n",
      "Epoch 890001, Cost: 21.51188441937428, m: 0.552371141715565, b: -25.68542071443301, cost: 21.51188441937428\n",
      "Epoch 900001, Cost: 21.40444422333999, m: 0.5538688946795373, b: -25.939314009886267, cost: 21.40444422333999\n"
     ]
    }
   ],
   "source": [
    "# Define the height and weight data\n",
    "X = np.array([152, 157, 163, 157, 160, 168, 173, 175, 178, 183,190])\n",
    "y = np.array([48, 53, 58, 56, 61, 66, 70, 73, 79, 86,88])\n",
    "\n",
    "\n",
    "# Initialize parameters\n",
    "m = 0  # initial guess for slope\n",
    "b = 0  # initial guess for y-intercept\n",
    "\n",
    "# Hyperparameters\n",
    "learning_rate = 0.00006\n",
    "num_epochs = 1000000\n",
    "\n",
    "# Number of data points\n",
    "n = len(X)\n",
    "\n",
    "# Gradient Descent\n",
    "for epoch in range(num_epochs):\n",
    "    # Calculate predictions\n",
    "    y_pred = m * X + b\n",
    "    \n",
    "    # Calculate the loss (MSE)\n",
    "    cost = (1 / (2 * n)) * np.sum((y_pred - y) ** 2)\n",
    "    \n",
    "    # Calculate gradients\n",
    "    gradient_m = (1 / n) * np.sum((y_pred - y) * X)\n",
    "    gradient_b = (1 / n) * np.sum(y_pred - y)\n",
    "    \n",
    "    # Update parameters using gradients and learning rate\n",
    "    m -= learning_rate * gradient_m\n",
    "    b -= learning_rate * gradient_b\n",
    "    \n",
    "    # Print loss for monitoring\n",
    "    if epoch % 10000 == 0:\n",
    "        print(f'Epoch {epoch + 1}, Cost: {cost}, m: {m}, b: {b}, cost: {cost}')\n",
    "\n",
    "# Final values of m and b\n",
    "print(f'Final values: m = {m}, b = {b},cost: {round(cost)}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda7716f",
   "metadata": {},
   "source": [
    "# Logistic Regression Mathematical Intution (Cost Function And Gradient Descent):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b640860c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, Cost: 0.6931471805599454\n",
      "Iteration 10001, Cost: 0.601583150171683\n",
      "Iteration 20001, Cost: 0.5505307863424463\n",
      "Iteration 30001, Cost: 0.5093853845889779\n",
      "Iteration 40001, Cost: 0.475833643314449\n",
      "Iteration 50001, Cost: 0.4481126575134082\n",
      "Iteration 60001, Cost: 0.4249037714979318\n",
      "Iteration 70001, Cost: 0.4052250952011625\n",
      "Iteration 80001, Cost: 0.38834277015284324\n",
      "Iteration 90001, Cost: 0.3737037276216073\n",
      "Final values: Theta = 1.488437989761718, Bias = -3.4729790288848634, cost: 0.3609\n"
     ]
    }
   ],
   "source": [
    "# Define the features and labels\n",
    "X = np.array([2.5, 1.5, 3.5, 2.0, 4.0, 3.0, 2.5, 3.0, 2.0, 1.0])\n",
    "y = np.array([1, 0, 1, 0, 1, 1, 0, 1, 0, 0])\n",
    "\n",
    "# Initialize parameters\n",
    "theta = 0  # Initial guess for weights (one weight for each feature)\n",
    "bias = 0  # Initial guess for bias term\n",
    "\n",
    "# Hyperparameters\n",
    "learning_rate = 0.0005\n",
    "num_iterations = 100000\n",
    "\n",
    "# Number of data points\n",
    "m = len(X)\n",
    "\n",
    "# Gradient Descent\n",
    "for iteration in range(num_iterations):\n",
    "    # Calculate predictions (logits)\n",
    "    z = X*theta + bias\n",
    "    y_pred = 1 / (1 + np.exp(-z))  # Using NumPy's exp function directly\n",
    "    \n",
    "    # Calculate the log loss (cross-entropy loss)\n",
    "    cost = (-1/m) * np.sum(y * np.log(y_pred) + (1 - y) * np.log(1 - y_pred))\n",
    "    \n",
    "    # Calculate gradients\n",
    "    gradient_theta = (1/m) * np.sum(X*(y_pred - y))\n",
    "    gradient_bias = (1/m) * np.sum(y_pred - y)\n",
    "    \n",
    "    # Update parameters using gradients and learning rate\n",
    "    theta -= learning_rate * gradient_theta\n",
    "    bias -= learning_rate * gradient_bias\n",
    "    \n",
    "    # Print loss for monitoring\n",
    "    if iteration % 10000 == 0:\n",
    "        print(f'Iteration {iteration + 1}, Cost: {cost}')\n",
    "\n",
    "# Final values of theta and bias\n",
    "print(f'Final values: Theta = {theta}, Bias = {bias}, cost: {round(cost, 4)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16beb9f6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
